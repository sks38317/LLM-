# 탐구 주제

생성형 언어 모델의 응답 생성 과정 분석: LLM(대규모 언어 모델)이 주어진 프롬프트(prompt)로부터 한 번의 응답을 만들어내는 전체 과정을 시간 순으로 살펴보고, 이 흐름 속에서 Softmax 함수, Temperature(온도)와 Top-p(nucleus sampling) 확률샘플링 기법, 그리고 모델 파인튜닝(fine-tuning)이 각각 어떠한 원리로 작용하는지 분석한다. 또한 이러한 생성 과정에서 흔히 나타나는 환각(hallucination) 현상의 발생 원인을 앞서 언급한 개념들과 연결지어 설명한다.

---

# 관련 과목 및 단원

- 수학: 확률과 통계 (확률 분포, 조건부 확률)
- 미적분 (극대값, 극소값 (최적화 개념))

---

# 탐구 주제 선정 이유

최근 ChatGPT와 같은 생성형 AI 모델이 교육, 비즈니스 등 다양한 분야에서 크게 주목받고 있다. 이러한 대규모 언어 모델(LLM)의 등장으로 AI가 자연어로 사람과 대화하고 글을 생성하는 시대가 열렸다. 하지만 AI가 어떻게 사람처럼 문장을 만들고 질문에 답하는지 그 내부 동작 원리는 복잡하게 느껴졌다. 학교 수학 시간에 배운 확률과 통계 개념(예: 확률 분포, 조건부확률)이 LLM의 원리에 사용된다는 이야기를 접하고, 직접 관련 개념들을 조사하여 LLM의 작동 과정을 이해해보고 싶었다. 특히 모델이 종종 사실과 다른 내용을 자신 있게 말하는 “환각 현상"은 모델의 한계이자 중요한 이슈이므로 어떤 원인으로 발생하는지 탐구해보기로 했다. 본 탐구를 통해 교과에서 배운 수학 개념들이 최첨단 AI 기술에 어떻게 적용되는지 알아보고자 한다.

---

# 이론적 배경 (수학 개념 및 원리)

## 언어 모델과 조건부 확률

LLM의 기본 임무는 다음에 올 단어 예측이다. 즉, 이미 주어진 단어 시퀀스(문맥)을 기반으로 다음 단어의 확률 분포를 계산하는 모델이다. 예를 들어 문맥이 “The capital of France is”라면, 인간은 자연스럽게 “Paris”를 예상하듯이, LLM도 훈련 데이터를 바탕으로 다음 단어로 “Paris”가 나올 확률이 매우 높음을 알고 있다. 이를 수학적으로 보면 "Paris"가 다른 단어들에 비해 조건부 확률이 비해 월등히 큰 것이다. 언어 모델은 이처럼 이전까지의 단어들이 주어졌을 때 각 가능한 다음 단어별 출현 확률을 예측한다. 그리고 문장 생성시 그 확률이 가장 높은 단어를 선택하거나, 확률에 따라 무작위로 샘플링하여 다음 단어를 출력한다. 이렇게 한 단어씩 이어 붙여가며 문장이 완성될 때까지 반복하는 것이 언어 모델의 기본 작동 원리이다.

## Softmax 함수와 확률 분포

언어 모델은 방대한 어휘 집합에 속한 모든 후보 단어들에 대해 예측 점수(score) 또는 로짓(logit) 값을 계산한다. 로짓은 해당 단어가 다음에 나올 가능성을 나타내는 모델의 내부 출력 값인데, 이 값들은 음수나 큰 양수 등 제한이 없고 합이 1로 정규화되어 있지도 않다. 따라서 각 단어의 로짓 값을 확률 분포로 바꾸어 주기 위해 소프트맥스(softmax) 함수를 사용한다. Softmax 함수란 여러 실수 입력값들을 입력받아, 이를 확률처럼 합이 1이 되는 K 개의 값으로 변환해주는 함수이다. 수식으로 표현하면 softmax의 출력 확률은
$$p_j = \frac{e^{z_j}}{\sum_{j=1}^{K} e^{z_j}}$$ 로 주어진다. 이 식을 통해 모든 로짓 값들이 0과 1 사이의 확률로 변환되고 총합이 1로 정규화된다. 로짓 값이 컸던 단어일수록 z 값이 크기 때문에 상대적으로 높은 확률을 갖게 된다. 이렇게 얻어진 확률 분포에서 확률이 가장 높은 단어는 가장 그럴듯한 다음 단어를 의미한다. 예를 들어 앞서 문맥이 “... 가장 좋아하는 색은”이었다면, LLM이 계산한 다음 단어 분포에서 “초록색”이 50%, “빨간색”이 30% 확률을 차지하는 식이다. 이 경우 “초록색”이나 “빨간색” 같은 단어들이 다음에 올 가능성이 높다고 모델이 판단한 것이다.

이러한 확률 분포 예측은 교차 엔트로피 손실(cross-entropy loss)을 통해 학습되었다. 모델이 예측한 확률 분포가 실제 정답 단어에 부여된 정답 분포(예: 정답 단어에 확률 1, 나머지에 0인 one-hot 분포)와 얼마나 차이가 나는지를 교차 엔트로피로 계산할 수 있다. 간단히 말해, 정답 단어에 모델이 부여한 확률이 높을수록 손실은 낮고, 틀린 단어에 높은 확률을 주었다면 손실이 크게 나온다. 예를 들어 실제 다음 단어가 “Paris”일 때 모델이 “Paris”에 0.9의 확률을, 엉뚱하게 “London”에 0.1의 확률을 할당했다면 손실은 비교적 낮겠지만, 거꾸로 “London” 0.9, “Paris” 0.1로 예측했다면 손실이 매우 커진다. 교차 엔트로피 손실 L 은 정답 단어의 확률값 text{correct}를 이용하여 $$L = -\log(p_{\text{correct}})$$ 처럼 계산할 수 있다. 언어 모델은 대용량 텍스트 코퍼스를 학습하면서 이 손실값을 경사 하강법(gradient descent) 등의 최적화 알고리즘으로 최소화하도록 훈련된다. 즉, 수많은 문장에서 다음 단어를 맞히는 예측을 반복하며 모델의 내부 가중치(parameters)가 업데이트되고, 그 결과 언어의 통계적 패턴을 학습하게 된다.

## 파인튜닝(fine-tuning)

기본적인 언어 모델(pre-trained LLM)은 인터넷 텍스트 등 방대한 일반 도메인 데이터를 통해 다음단어 맞히기 능력을 익혔다. 이렇게 학습된 모델을 실제 응용 분야에 활용하려면, 해당 특정 작업이나 스타일에 맞게 추가 훈련시키는 과정이 필요할 때가 많다. 이를 파인튜닝이라고 부른다. 파인튜닝은 이미 학습된 거대 모델이 가진 언어 지식을 바탕으로 소량의 추가 데이터만으로 모델을 특정 목적에 맞게 세밀 조정하는 것이다. 예를 들어 대화형 AI 비서를 만들기 위해, 기본 언어 모델에 여러 "사람 질문과 모범 답변 쌍"들을 학습시켜 사용자의 지시에 답하는 방식을 가르칠 수 있다. 또, 모델의 말투나 어조를 바꾸고 싶다면 원하는 스타일의 텍스트로 추가 훈련시킬 수 있고, 전문 지식이 필요한 분야라면 해당 분야의 데이터로 파인튜닝하여 도메인 지식을 보강할 수도 있다. 파인튜닝도 결국 앞서 언급한 교차 엔트로피 등의 손실 함수를 최소화하는 방향으로 모델의 가중치를 업데이트하는 과정이다. 다만 파인튜닝은 일반적으로 학습률을 작게 하고 훈련 데이터 양도 제한하여, 기존의 일반 언어 능력은 유지하면서도 필요한 특정 기능이나 제약을 모델에 부여하는 것을 목표로 한다. 특히 ChatGPT와 같은 모델은 지도 학습 미세조정(supervised fine-tuning)뿐만 아니라 인간 피드백을 활용한 강화학습(RLHF) 등 복합적인 파인튜닝 과정을 거쳐, 사용자 지시를 최대한 따르고 유해한 발언을 피하는 등 안전하고 도움되는 응답을 생성하도록 훈련되어 있다.

## 생성 샘플링 방법과 Temperature, Top-p

언어 모델이 다음 단어의 확률분포를 예측하면, 실제로 그 중 어느 단어를 선택해 출력할지를 정해야 한다. 가장 단순한 방법은 탐욕적 선택(greedy)으로 확률이 가장 높은 단어를 고르는 것이다. 이 경우 항상 최빈단어만 고르게 되어 결과 문장이 매번 동일하고 평범하거나 반복적인 응답이 될 수 있다. 창의적인 문장 생성을 위해서는 확률적으로 무작위성을 도입해볼 수 있다. 예를 들어 어떤 문맥에서 한 단어가 70% 확률, 다른 단어가 30% 확률이라면, 매번 70%짜리만 택하기보다는 그 비율에 따라 랜덤하게 선택하면 출력의 다양성이 높아진다. 이렇게 확률 분포에 따라 샘플링하여 다음 단어를 고르는 과정을 생성(decoding) 또는 샘플링(sampling)이라고 한다. 다만 완전히 확률에만 의존하면 가끔 어색하거나 말이 안 되는 단어가 뽑힐 수도 있으므로, 이를 통제하기 위한 여러 기법이 고안되었다. 대표적인 두 가지 샘플링 조절 매개변수, 온도(temperature)와 상위 확률(top-p) 설정이다.

## Temperature(온도)

T(temperature)는 분포의 평탄도(엔트로피)를 조절하는 변수로, 모델이 생성하는 무작위성의 정도를 결정한다. Softmax에 temperature를 적용하는 원리는, 로짓 값들을 Softmax로 확률화하기 전에 1/T 만큼 스케일링하는 것이다. 예를 들어 T=1이면 일반 Softmax와 동일하고, T<1 (예: 0.5)이면 로짓을 큰 값으로 나누는 효과가 생겨 확률 분포가 더욱 첨예해진다. 특히 가장 높은 확률의 단어가 더욱 높은 확률로 부각되고, 나머지 후보들은 확률이 더 낮아져 모델이 가장 그럴듯한 단어에 집중하게 된다. 반대로 T>1 (예: 2)으로 크면 로짓을 작은 값으로 나누는 것이므로 원래의 확률 격차를 줄여 분포를 평평하게 만든다. 이렇게 하면 여러 후보들의 확률값이 서로 비슷해지므로, 비교적 덜 흔한 단어들도 선택될 여지가 커지고 출력의 다양성이 증가한다. 요약하면 T 가 낮을수록 결과가 확실하고 일관적이게 되며, T 가 높을수록 결과가 창의적이고 예측 불가능해진다. 극단적으로 T 가 0에 가까우면 매번 확률이 가장 높은 단어만 뽑게 되어 항상 동일한 응답을 내놓지만, T 가 2에 가까우면 아무 말이나 하는 것처럼 횡설수설할 가능성도 높아진다.

## Top-p (상위 확률) 샘플링

Top-p는 누적 확률 상위 p 비율에 해당하는 작은 후보 집합만을 고려하여 그 중에서만 다음 단어를 뽑는 기법이다. 이를 누클리어스 샘플링(nucleus sampling)이라고도 한다. 구체적으로, 모델이 Softmax로 구한 확률값들을 내림차순으로 정렬하고, 위에서부터 확률을 더해 나가 p (예: 0.9) 이상이 될 때까지의 단어들만 후보로 유지한다. 그리고 그 핵심 후보군 내에서만 확률을 다시 정규화해 랜덤 선택을 한다. 이렇게 하면 확률이 너무 낮은 비현실적인 후보들은 아예 배제 하므로 말이 안 되는 출력이 나올 위험을 줄일 수 있다. 예를 들어 어떤 문맥에 대해 모델이 예측한 다음 단어 분포가 다음과 같다고 하자.

분포에서 ‘yes’의 확률이 60%, ‘maybe’가 31%, ‘no’가 8%, 그 밖의 기타 단어들이 나머지 1%라면, Top-p를 0.9로 설정할 경우 ‘yes’(60%)와 ‘maybe’(31%) 두 개의 후보만으로 누적 확률 91%에 도달하므로 이 둘만 다음 단어 후보로 취급한다. 나머지 확률이 낮은 단어들은 제외하고, 이 핵심 후보들 중에서만 최종 출력을 무작위 선택하게 된다. 반면 Top-p를 0.99로 높이면 ‘yes’+‘maybe’+‘no’의 누적 확률이 99%가 되어 ‘no’까지 후보에 포함된다. 일반적으로 Top-p 값은 0.8 ~ 0.95 정도로 설정하여 너무 터무니없는 후보는 걸러내되, 일정 수준의 다양성은 확보하는 절충을 한다. Top-p를 1로 두면 모든 단어를 그대로 고려하므로 완전한 샘플링과 같고, Top-p를 아주 낮게 (예: 0.1) 두면 상위 몇 개만 남겨서 거의 결정론적 생성에 가까워진다.

이처럼 Temperature와 Top-p는 모델의 출력 문장이 단조롭지 않고 풍부하면서도 너무 파괴적으로 엉뚱하지 않게 만드는 역할을 한다. 적절한 값을 찾기 위해서는 실험이 필요하며, 둘을 함께 활용하여 모델 출력을 세밀하게 제어할 수도 있다.

---

# LLM 응답 생성 흐름과 각 개념의 역할

이제 앞서 설명한 개념들이 실제 언어 모델이 답변을 생성하는 과정에서 어떤 순서로 개입되고 영향을 미치는지, 단계별로 정리한다. 아래에서는 하나의 질문에 대해 LLM이 한 문장을 만들어내는 한 번의 응답 생성 과정을 처음부터 끝까지 따라가며 살펴본다 (이때 OpenAI GPT 계열 모델의 작동을 참고한다). 각 단계에서 Prompt, Softmax, Temperature, Top-p, Fine-tuning이 어떤 역할을 하는지도 함께 설명한다.

## 1. 프롬프트 입력과 문맥 처리

사용자가 질문이나 요청을 입력하면 그것이 프롬프트(prompt)가 된다. 예를 들어 프롬프트가 “지구에서 달까지의 거리은 얼마인가?” 라고 하자. LLM은 우선 이 프롬프트 문장을 토큰화(tokenization)하여 내부적으로 숫자 목록인 입력 시퀀스로 변환한다. 모델은 이 입력을 인식하고 이해하는데, 이때까지 모델 내부에는 방대한 훈련을 통해 형성된 가중치 파라미터들과 어휘 사전이 준비되어 있다. LLM이 프롬프트를 받으면 사전 학습과 파인튜닝을 통해 형성된 지식을 동원하여, 질문의 의미를 파악하고 적절한 답을 생성할 준비를 한다. 파인튜닝된 모델일 경우, 예를 들어 지도학습 미세조정으로 질의-응답 형식에 길들여져 있다면, 질문에 답하는 어조와 형식을 따르도록 이미 학습되어 있다. 따라서 프롬프트 입력 단계에서 파인튜닝의 효과는 모델이 사용자 요청을 이해하고 어떤 종류의 답변을 해야 할지 방향을 잡는 역할을 한다. 즉, 같은 내용의 질문이라도 파인튜닝 여부에 따라 답변 어조가 공손해진다거나, 주어진 지시(학술적 어조/비속어 사용 금지 등)를 더 잘 지키는 등의 차이가 나타난다. 이후 LLM은 프롬프트를 컨텍스트로 하여 다음 단어를 예측하는 디코딩(decoding) 과정을 시작한다.


## 2. 다음 단어 예측 및 Softmax 확률 계산

모델의 디코더(decoder)는 현재까지의 모든 단어들을 고려해 다음에 올 단어들의 점수, 즉 로짓(logit)을 계산한다. 이 계산은 "Transformer"와 같은 신경망 구조에서 이루어지며, 프롬프트의 의미와 문법적 맥락 등을 반영한 복잡한 함수 연산의 결과이다. 로짓 값은 예컨대 “3841번 단어: 7.2, 930번 단어: -1.3, ...” 와 같이 모델 어휘 사전의 모든 단어별로 하나씩 얻어진다. 그리고 이 로짓 벡터에 Softmax 함수를 적용하여 각 단어가 다음에 등장할 확률 분포로 변환한다. 이때 파인튜닝은 모델의 로짓 분포 자체에 영향을 주는데, 이는 곧 특정 단어들의 확률을 더 높이고 낮추는 효과로 이어진다. 예를 들어 모델이 파인튜닝을 통해 “모르겠어요”처럼 답을 회피하는 패턴을 학습한 경우, 모르는 질문에 대해 “모릅니다”와 같은 출력 토큰의 로짓을 상대적으로 높게 산출할 수 있다. Softmax 처리 결과, 모든 후보 단어들은 0에서 1 사이의 확률값을 갖게 되며 합은 1이 된다. 이렇게 얻어진 다음 단어 확률분포에서 어떤 단어는 높은 확률(예: 0.6 = 60%), 어떤 단어는 극히 낮은 확률(예: 0.0001 미만)을 가질 것이다. 우리 예시 질문 “지구에서 달까지의 거리”의 경우, 상위 후보로 “약”, “約”, “는” 등이 일정 확률로 나오고 그 뒤 “38만 km”와 같은 숫자 토큰 시퀀스의 시작이 후보에 포함될 수 있다. 이 단계까지의 모델 상태를 요약하면: 프롬프트를 조건으로 “다음에 올 가능성이 있는 모든 단어의 확률분포”가 계산된 것이다.

예시: 한 언어 모델에 “What’s your favorite color? (가장 좋아하는 색은?)”이라는 프롬프트를 입력했을 때  해당 모델은 다음 단어로 나올 수 있는 후보들과 그 확률 분포를 'green' (초록) 약 50%, 'red' (빨강) 약 30% 등의 확률을 부여하였다. 이러한 분포는 소프트맥스 함수를 통해 얻어진 것으로, 확률 값이 높을수록 다음에 그 단어가 등장할 가능성이 크다는 것을 의미한다.

## 3. Temperature를 통한 확률 분포 조정

Softmax로 얻은 기본 확률분포에 Temperature 값을 적용하여 분포의 Sharpness(뾰족함)를 조절한다. 앞서 이론적 배경에서 설명했듯이, T(temperature)는 확률분포의 엔트로피를 조절하는 파라미터이다. 실전에서는 로짓 벡터를 Softmax에 넣기 전에 T 로 나누어주는데, 이는 $$\frac{x_i}{T}$$ 로 로짓을 스케일링하는 것과 같다. T<1이면 큰 로짓들은 더 커지고 작은 로짓들은 더욱 작아져서 가장 높은 확률의 단어에 더욱 확신을 갖는 분포로 바뀐다. 반대로 T>1이면 로짓 값들이 균등해져 확률분포가 한층 평탄해진다. 예를들어 기본 Softmax 결과 상위 후보들이 “약 (0.4), 38 (0.2), 38만 (0.1), ...” 등의 확률을 갖고 있었다면, T=0.5처럼 낮출 경우 “약”의 확률이 더욱 높아지고 다른 후보들은 거의 0에 수렴하게 될 수 있다. 이렇게 하면 모델은 가장 가능성 높은 출력에 집중하게 되어 결정론적인 답변에 가까워진다. 반대로 T=2.0처럼 높이면 “약”, “38”, “38만” 등이 보다 고른 확률을 갖게 되어, 무작위적 요소가 증가한다. Temperature 조정은 질문의 성격에 따라 적절한 값을 선택하는데, 예를 들어 상세한 설명이 필요한 질문에서는 약간 높은 T 로 창의적인 표현을 유도하고, 수치나 사실을 묻는 질문에서는 T 를 낮춰 정확도를 높이는 식으로 활용한다. Temperature 값 적용 후에는 다시 확률값을 정규화하여 최종 분포를 얻는다 (수식적으로 Softmax에 $$\frac{z_i}{T}$$ 를 대입한 것과 동일하다). Temperature 단계를 거친 후에도 분포의 최고 확률 단어 순위 자체는 바뀌지 않지만, 확률 격차가 변동되어 랜덤 샘플링 시 선택될 가능성의 상대적 비율에 변화를 준다.

## 4. Top-p에 따른 후보 필터링

다음으로 Top-p (누클리어스) 샘플링 기준을 적용해 확률이 너무 낮은 후보 단어들을 제외한다. 우선 Temperature 적용 후의 확률분포에서 확률이 높은 순으로 단어들을 나열하고, 누적 확률을 계산한다. Top-p값은 누적 확률의 임계치로서, 예를 들어 p=0.9라면 위에서부터 확률을 더하여 합계가 0.9 이상이 되는 지점까지의 단어들만 선택한다. 이 작은 후보 집합이 핵심 후보들(nucleus)이 되며, 나머지 10% 확률에 해당하는 낮은 후보들은 잘라낸다. 예를들어 “약 (0.5), 38 (0.25), 38만 (0.15)” 세 단어의 누적합이 0.9를 넘는다면 이 셋만 고려하고, 그 이외 “킬로미터” 등의 나머지 후보들은 제외한다. 이 과정을 통해 모델은 가능성이 매우 낮은 이상한 단어들을 걸러내므로, 이후 무작위 선택 과정에서 비상식적 출력이 섞일 확률을 낮춘다. Top-p를 1.0으로 설정하면 필터링을 하지 않는 것이고, 1.0보다 작게 설정할수록 후보 풀(pool)을 좁혀 더 보수적인 생성을 하게 된다. 이 값은 사용자가 응답의 다양성 vs. 안전성 간에 원하는 지점을 조절하는 역할을 하며, 일반적으로 0.8 ~ 0.95 사이에서 많이 설정한다. Top-p 필터링 후, 선택된 핵심 후보들에 대해 다시 한 번 확률을 정규화(나머지 제외된 만큼을 빼고 합이 1이 되도록)한다.

## 5. 다음 단어 샘플링 및 반복

이제 최종 후보 단어들의 확률분포에서 한 개의 단어를 무작위 추출한다. 이 추출은 확률적 샘플링으로 이루어지며, 예컨대 핵심 후보가 3개라면 각각의 확률 비율에 따라 랜덤하게 한 단어를 선택한다. 확률이 가장 높은 후보라도 100% 선택되는 것은 아니지만, 여전히 가장 높은 확률을 가진 단어일수록 뽑힐 가능성이 크다. 반면 확률이 낮은 단어는 드물게 나오지만 그래도 배제는 되지 않았기에 가끔 선택되어 의외의 전개를 만들기도 한다. 이렇게 선택된 단어가 모델의 출력 결과에 이어 붙는다. 그리고 모델은 그 단어를 입력 문맥에 추가한 새로운 컨텍스트로 삼아 다시 다음 단어에 대한 확률분포를 계산한다. 위의 2 ~ 4단계를 반복하여 문장을 이어나가는 것이다. 이 과정은 모델이 종료 조건에 도달할 때까지 계속된다. 종료 조건으로는 보통 (a) 미리 정해둔 최대 토큰 길이에 도달하거나, (b) 모델이 특수 종료 토큰(예: <EOS>)을 출력하거나, (c) 모델이 더 이상 유의미한 출력을 내놓지 못할 때 등이 있다. 우리의 예시 질문 “지구와 달 거리”의 경우 모델은 “약 38만 4천 km입니다.”라는 완전한 답변을 생성하고 <EOS> 토큰을 내보내어 종료되었다고 가정하자. 이렇게 하나의 응답 생성 사이클이 마무리된다. LLM은 질문에 답하기 위해 프롬프트 처리 → 로짓 계산 및 Softmax 확률화 → Temperature 조절 → Top-p 필터링 → 확률적 샘플링 → 출력 및 반복이라는 단계들을 거치는 것이다. Fine-tuning은 이러한 전 과정에 스며들어 모델의 출력 어조나 내용 스타일을 결정짓고, Softmax 확률 분포 자체에도 영향을 주어 모델이 사용자 의도에 부합하는 방향으로 답변하게 도와준다.

---

# 환각(hallucination) 현상의 발생 원인 및 각 개념과의 연관성

“환각”이란 LLM이 실제 사실과 맞지 않거나 근거 없는 정보를 그럴듯하게 지어내는 현상을 가리킨다. 예를 들어 실제로 존재하지 않는 참고 문헌을 만들어내거나, 모르는 내용에 대해 근거 없이 추측으로 답변해버리는 경우가 이에 해당한다. 이러한 환각은 왜 발생하는 것일까? 앞서 살펴본 출력 생성 흐름과 개념들의 측면에서 그 원인을 정리하면 다음과 같다.

## 프롬프트와 환각

모델은 오로지 프롬프트(질문 내용)와 훈련된 확률에 의존해 답변을 만든다. 만약 프롬프트에 필요한 정보가 충분히 주어지지 않았거나 질문 자체가 모델이 접하지 못한 내용에 대해 묻는 경우, 모델은 훈련 데이터상 가장 그럴듯한 추론을 통해 답을 만들어낼 수밖에 없다. 즉, 알지 못하는 것을 모른다고 말할 능력이 본질적으로 없고, 주어진 단서 내에서 가장 확률 높은 말을 이어갈 뿐이다. 이때 프롬프트가 애매하거나 함정이 있다면 모델은 맥락을 잘못 추론하여 엉뚱한 정보를 사실인 양 생성할 수 있다. 다시 말해, 환각은 흔히 프롬프트에 정보 부재 또는 잘못된 정보가 있을 때 촉발된다. 사용자가 “~ 에 대한 자세한 내용을 알려줘”라고 요구하지만 정작 그런 내용이 훈련에 없었다면, 모델은 질문 속 단어들을 단서로 관련이 있어 보이는 임의의 내용을 창작해낼 수 있다.

## Softmax 확률 및 모델의 확신

Softmax 단계에서 모델은 어떠한 상황에서도 다음에 나올 단어에 대한 확률분포를 산출한다. 이 분포는 결국 모델이 가장 “그럴 듯한” 답변을 선택하도록 이끈다. 문제는, 그럴 듯함이 꼭 진실과 같지 않다는 데 있다. 모델은 지식 기반에 근거해 확률을 산출하지만, 그 지식 자체가 잘못되었거나 불완전하면 틀린 내용에도 높은 확률을 줄 수 있다. Softmax는 가장 확률 높은 값을 선택(또는 샘플링)하도록 하므로, 모델이 확신은 하지만 틀린 답을 내놓을 위험이 있다. 특히 지식이 애매한 질문의 경우, 모델은 훈련 데이터에서 언어적으로 많이 등장한 패턴을 따를 가능성이 크다. 예컨대 “~ 의 발명자는 누구인가?”라는 물음에 대해 정답을 모르면, Softmax 확률은 데이터 상 자주 등장하는 아무 이름이나 높게 배정할 수 있고, 모델은 그것을 선택해 답해버릴 수 있다. 이는 마치 인간이 모르는 질문에 아는 척으로 답변을 꾸며내는 것과 비슷해 보이지만, 실제로는 모델이 “모른다”는 개념 없이 항상 어떤 출력은 내야 한다는 구조적 한계 때문에 발생한다. 결국 Softmax에 의해 어떤 토큰이든 출력은 되기 마련이므로, 그 토큰이 사실과 다르면 그대로 환각이 현실화된다. 다시 말해 LLM의 확률적 본질 자체가 환각을 내포하고 있는 셈이다.

## Temperature 설정

Temperature(T)가 높을수록 모델 출력에 무작위성이 커지고 비정형적 응답이 나올 가능성이 높다. 이는 창의성을 높여주는 장점이 있지만, 반대로 사실과 동떨어진 말이 나올 확률도 높여줄 수 있기 때문이다. 예를 들어 낮은 T 에서는 “파리는 프랑스의 수도”처럼 뻔하지만 정확한 답을 했다면, 높은 T 에서는 뜬금없는 도시 이름을 언급하거나 “정확한 수도는 현재 논쟁 중” 같은 근거 없는 창작을 할 수도 있다. 따라서 T 를 너무 높게 설정하면 환각 확률이 증가할 수 있다. 한편 T 를 너무 낮게 하면 항상 확률 1위만 취하므로 오히려 잘못 학습된 오답을 반복하는 문제가 생길 수 있다. 예컨데 모델이 어떤 사실을 잘못 기억하고 있어서 그 오답에 높은 확률을 할당했다면, 낮은 T 에서는 매번 그 오답만 출력하게 되어 지속적으로 잘못된 정보를 확신 있게 내놓을 것이다. 요컨대 Temperature는 환각에 양면적 영향을 미치는데, 높은 T 는 창의적 환각(엉뚱한 새로운 거짓)을 야기할 수 있고, 낮은 T 는 확신에 찬 환각(틀린 내용을 반복)으로 이어질 수 있다. 따라서 환각을 감소시키기 위해선 적절한 Temperature조절을 해야한다.

## Top-p 설정

Top-p는 아주 낮은 확률의 토큰을 제거하므로 직관적으로 환각 감소에 도움이 된다. Top-p 값이 낮을수록 모델은 보편적인 답변만 하게 되고 괴짜 같은 말은 하지 않게 된다. 예를 들어 상식적으로 맞지 않는 단어나 문장은 기본 확률 자체가 낮게 나오기 마련이므로, Top-p=0.9 등으로 제한하면 그런 것들은 걸러질 확률이 높다. 그러나 Top-p를 너무 낮게 잡으면 매번 똑같은 식상한 답만 나오거나 문장이 부자연스러워질 수 있다. 또한 환각이라는 것이 꼭 극단적으로 낮은 확률에서만 나오지는 않는다. 모델이 아예 모르면서 그럴듯하다고 착각한 정보는 오히려 비교적 높은 확률로 분포 상위에 있을 수 있다. 그런 경우엔 Top-p로도 걸러지지 않고 그대로 출력되어 환각이 발생한다. 한편, Top-p를 높여 (예: 1.0에 가깝게) 다양한 후보를 허용하면 특이하고 사실과 안 맞는 출력이 섞일 가능성을 키운다는 점에서 환각 위험이 증가할 수 있다. 따라서 Top-p 역시 너무 높이면 모델이 잠재적으로 엉뚱한 토큰까지 선택할 수 있어 환각 확률이 높아진다.

## 파인튜닝과 모델의 한계

파인튜닝은 일반적으로 LLM의 지식 총량을 크게 늘리지는 않는다. 즉, 모델이 알지 못했던 새로운 사실을 소수의 파인튜닝 데이터만으로 학습시키기는 어렵다. 그래서 모델이 모르는 질문에 직면하면, 파인튜닝을 했더라도 여전히 훈련된 대로 그럴듯한 언어 출력을 내놓을 뿐이다. 다만 파인튜닝의 유형에 따라 환각에 미치는 영향이 다를 수 있다. 지식 추가형 파인튜닝(예: 의료 코퍼스로 추가 학습)은 해당 분야 환각률을 줄일 수 있지만, 대화형 파인튜닝(예: 친절하게 답하는 법을 학습)은 오히려 모델로 하여금 “질문에 반드시 답해야 한다”는 경향을 심어줄 수 있다. 예컨대 RLHF(Reinforcement Learning from Human Feedback)를 통해 사용자가 질문하면 웬만해선 답변을 하도록 보상했다면, 모델은 모르는 내용이라도 무언가 답변을 만들어내는 쪽으로 강화되었을 수 있다. 이는 모델이 “모르면 추측하지 말고 모른다고 답하기”를 충분히 학습하지 않았다면 부작용으로 작용하여, 모르면서도 답변을 지어내게 만들 가능성이 있다. 이를 정렬(alignment) 문제가 부족한 사례로 볼 수 있다. 예를 들어 어떤 사용자가 특정 식이제한(글루텐 알레르기)이 있다고 프롬프트에 밝혔는데도, 파인튜닝이 덜 된 모델은 그런 제약을 제대로 반영하지 못하고 흔한 식단 조합(토스트 등)을 제안해버릴 수 있다. 이 경우 모델은 사용자의 조건을 놓치고 훈련 데이터 상 일반적인 아침식사 메뉴를 답한 것인데, 사용자는 이를 모델의 사실 오류(환각)로 받아들이게 된다. 이러한 정렬 오류로 인한 환각은 모델이 지식을 몰라서라기보다, 지시를 정확히 따르지 못해서 발생한 경우라 할 수 있다. 요컨대, 파인튜닝은 모델의 출력을 사람 의도에 맞게 최적화하지만, 그 범위를 벗어난 경우에는 여전히 모델은 확률적으로 그럴듯한 (하지만 틀린) 응답을 내놓을 수 있다. 완벽한 환각 방지를 위해서는 향후 모델에 사실 검증이나 지식 그래프 참조 등의 추가적인 장치가 필요하며, 이는 현재 활발한 연구 주제다. 결국 LLM의 환각 현상은 모델의 확률적 생성 특성과 지식 한계에서 기인하며, Softmax 기반의 다음 단어 예측 구조 자체가 항상 잠재적 환각을 내포하고 있다고 볼 수 있다. 우리는 Temperature나 Top-p 조절, 그리고 보다 철저한 파인튜닝/정렬 등을 통해 그 빈도를 낮출 수는 있지만, 완전히 제거하기는 어렵다는 점이 많은 연구자들의 견해이다.

---

# 활동을 통해 느낀 점 및 생각의 변화

이번 탐구 활동을 통해 평소 마주하던 ChatGPT와 같은 생성형 AI의 속내를 수학적 원리로 풀어보는 값진 경험을 했다. 처음에는 사람이 언어를 다루는 방식과 너무도 유사하게 느껴져 마치 “생각하면서 말하는 존재”로 착각하기 쉬웠는데, 실제로는 모든 과정이 확률과 함수 계산의 연속임을 알게 되었다. 거대한 신경망 모델이 조건부 확률을 예측하고 그에 따라 다음 단어를 선택하는 과정을 하나하나 분석하니, 겉보기에는 신비롭게 보이던 AI의 언어 능력이 결국 수학적인 원리들의 집합체라는 사실이 명확해졌다. 특히 Softmax 함수나 교차 엔트로피 손실 등은 교과 수학이나 통계 시간에도 배우는 개념인데, 이런 것들이 AI 언어 모델의 핵심이었다니 매우 놀라웠다.

또 한 가지 깨달은 점은, AI 모델이 똑똑해 보여도 오류를 생성할 수밖에 없는 구조적 이유가 있다는 것이다. 모델의 응답 생성 흐름을 추적하며 왜 환각이 발생하는지 이해하게 되니, 앞으로 AI의 답변을 볼 때도 무조건 신뢰하기보다는 비판적으로 검증해야겠다는 생각이 들었다. 예를 들어 모델이 그럴듯하게 말하는 정보도 알고 보면 단순히 훈련 데이터의 확률적 산물일 수 있으며, 내가 원하는 정확한 답이 아닐 수 있다는 점을 늘 염두에 둘 것이다. 반대로, 환각 문제를 완화하기 위해 연구자들이 왜 지식 기반 연결, 사실 검증, 정렬 강화 등의 기법을 개발하는지도 더 잘 이해하게 되었다.

종합적으로, 이번 탐구를 통해 AI 언어 모델의 작동 원리를 수학적으로 탐구하며 이론과 실제의 연결고리를 찾을 수 있었다는 점이 의미 있었다. 확률 분포, 최적화, 함수 등이 어떻게 현실의 AI에 응용되는지 배우면서 학교에서 배운 수학의 가치와 재미를 새삼 느꼈다. 나아가 AI를 활용할 때 장단점을 균형 있게 바라보는 시각도 갖추게 되었으며, 장차 이러한 분야를 더 깊이 공부하고 싶다는 동기도 얻었다.
